---
title: Regularização l1 e l2
excerpt: "Pra que e por que dividir meu dataset em teste e treino?"
header:
  image: /assets/images/treino.jpg
categories: [intuition]
tags: [l1/l2]
---

oq eu sei até agora é q são regularizações pra penalizar modelos complexos


tipo na função objetivo se vc só quer minizar vai parar com um modelo complexo bagarai e tal, aí começaram a colocar a complexidade do modelo como restrição pra balancear isso

essa parte vc sabe


l1 e l2 são dois jeitos de fazer isso


l1 é mais barra pesada

e l2 é mais suave

quanto maior o número desses hiperparametros mais penalidade pra o modelo complexo

tipo no xgboost o alpha é o l1 pra peso de leafs

se vc botar um alpha grande

ele vai dar peso 0 pra um bando de leaf

pq ele vai penalizar o modelo ter muita leaf

ou ter mt peso de leaf

e um modelo com muito peso de leaf é mais complexo


----------------------------------------
asta escrever o código a seguir:


```python
import pandas as pd

train = pd.read_csv('train.csv')

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(train.drop('Survived',
                                                    axis=1),
                                                    train['Survived'],
                                                    test_size=0.3,
                                                    random_state=42)
	
```

Nesse código importamos a biblioteca pandas na primeira linha. Na segunda, pedimos que ela leia o arquivo train.csv e transforme num Data Frame com nome train.

Na terceira linha importamos a função train_test_split do módulo model_selection da biblioteca scikit-learn. Na quarta linha associamos quatro variáveis ao resultado da função.

Você pode olhar a [documentação da função][documentacao do traintest split] para ver os detalhes, mas resumidaemente ela vai dividir o seu DataFrame em duas partes, a de teste e a de treino (já vou explicar pq isso gera 4 dfs). Como você pode ver os dois primeiros parâmetros da função são o dataset sem a coluna de interesse (por isso usamos o .drop que tira a coluna 'Survived') e a coluna de interesse separada do resto dos dados.

Isso é apenas pro scikit entender qual é a coluna de interesse. O que ele vai fazer é dividir seus dados em duas partes: 70% para um treino, 30% para teste. Você pode ajusta isso no parâmetros test_size. Podemos falar mais sobre essas proporções no futuro.

## Não se confunda!

A ordem que escrevemos o código costumava me confundir um pouco sobre o que eu tinha que treinar e o que testar. Nós vamos treinar o x_train usando o y_train, ou seja, vamos falar pro algoritmo q o y_train é a coluna de sobreviventes das pessoas no x_train. Depois vamos dar o predict no y_train, o algoritmo vai predizer quem do y_train morreu ou viveu, e comparar o resultado (y_predicted) com o y_test, que é a "resposta certa" para calcular os scores.

Nos próximos posts falaremos melhor sobre como importar os dados, sobre o desafio do titanic em si, e sobre como treinar e testar modelos de Machine Learning. Até lá!

[documentacao do traintest split]:http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
[desafio do titanic]:https://www.kaggle.com/c/titanic